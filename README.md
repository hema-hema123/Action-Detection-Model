#  ğŸ§  Action Detection Model ğŸ¬

A deep learning model built using **MediaPipe**, **TensorFlow**, and **OpenCV** to detect and classify human actions in real-time through webcam or video input.

## ğŸš€ Features

- Real-time pose estimation using **MediaPipe**
- Action classification using a trained LSTM model
- Visual feedback with OpenCV overlay
- Modular code for training and testing
- Works on live webcam feed and pre-recorded videos

---

## ğŸ–¥ï¸ Tech Stack

- **Frontend (Visualization)**: OpenCV
- **ML Libraries**: TensorFlow, NumPy, MediaPipe
- **Backend**: Python
- **Notebook**: Jupyter Notebook
- **Testing**: Matplotlib, NumPy
- **Deployment**: Local machine
## ğŸ“ Project Structure

Action-Detection-Model/
â”œâ”€â”€ Action Detection model.ipynb # Jupyter notebook for training/testing
â”œâ”€â”€ model.h5 / action.keras # Trained model
â”œâ”€â”€ 0.npy # Extracted pose keypoints
â”œâ”€â”€ data/ # Raw and processed data
â”œâ”€â”€ Logs/ # TensorBoard logs
â”œâ”€â”€ utils.py # Utility functions (if any)
â”œâ”€â”€ README.md # You're here!

yaml
Copy
Edit
ğŸ§  Model Training
Pose landmarks are extracted using MediaPipe

Sequences of frames are converted to NumPy arrays

LSTM model is trained to classify actions like walking, waving, etc.

Saved using .h5 or .keras formats

